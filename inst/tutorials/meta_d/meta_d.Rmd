---
title: "Doing meta-analysis in R: Cohen's *d*"
author: "Andy Field"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
    theme: "united"
    css: ./css/discovr_style_future.css
runtime: shiny_prerendered
description: "A tutorial on conducting a meta-analysis in R using the `metafor` package with effect size Cohen's *d*."
bibliography: [meta_bib.bib, packages.bib]
---
<html lang="en">

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

#necessary to render tutorial correctly
library(learnr) 
library(htmltools)
#tidyverse
library(dplyr)
library(ggplot2)
#non tidyverse
library(broom)
library(DT)
library(metafor)
library(knitr)

source("./www/discovr_helpers.R")


#Read dat files needed for the tutorial

brewin_2024 <- metahelpr::brewin_2024
brewin_es <- metahelpr::brewin_es


tbl_font_size <- "12pt"


# create some global tibbles

brewin_agg <- brewin_es |> 
  dplyr::group_by(author) |> 
  dplyr::summarize(
    g = mean(g, na.rm = T),
    v_g = mean(v_g, na.rm = T),
    n = mean(n_total, na.rm = T)
  )

brewin_foa_agg <- brewin_es |> 
  dplyr::group_by(foa_gp, author) |> 
  dplyr::summarize(
    g = mean(g, na.rm = T),
    v_g = mean(v_g, na.rm = T),
    n = mean(n_total, na.rm = T)
  )
```

```{r, eval = F, echo = F}
# Create bib file for R packages
here::here("inst/tutorials/meta_d/packages.bib") |>
  knitr::write_bib(c('here', 'tidyverse', 'dplyr', 'DT', 'readr', 'forcats', 'tibble', 'knitr', 'broom', 'metafor'), file = _)
```


# metahelpr: Meta-analysis using Cohen's *d*/Hedges' *g*

## Overview

<div class="infobox">
  <img src="./images/discovr_hex.png" alt="discovr package hex sticker, female space pirate with gun. Gunsmoke forms the letter R." style="width:100px;height:116px;" class = "img_left">
  
  **Usage:** This tutorial is offered under a [Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License](http://creativecommons.org/licenses/by-nc-nd/4.0/). Tl;dr: you can use this tutorial for teaching and non-profit activities but please don't meddle with it or claim it as your own work.
  
</div>

### `r cat_space(fill = blu)` Welcome to the `discovr` space pirate academy

Hi, welcome to **discovr** space pirate academy. Well done on embarking on this brave mission to planet `r rproj()`s, which is a bit like Mars, but a less red and more hostile environment. That's right, more hostile than a planet without water. Fear not though, the fact you are here means that you *can* master `r rproj()`, and before you know it you'll be as brilliant as our pirate leader Mae Jemstone (she's the badass with the gun). I am the space cat-det, and I will pop up to offer you tips along your journey.

On your way you will face many challenges, but follow Mae's system to keep yourself on track:

* `r bmu(height = 1.5)` This icon flags materials for *teleporters*. That's what we like to call the new cat-dets, you know, the ones who have just teleported into the academy. This material is the core knowledge that everyone arriving at space academy must learn and practice. For accessibility, these sections will also be labelled with [(1)]{.alt}.
* `r user_visor(height = 1.5)` Once you have been at space pirate academy for a while, you get your own funky visor. It has various modes. My favourite is the one that allows you to see everything as a large plate of tuna. More important, sections marked for cat-dets with visors goes beyond the core material but is still important and should be studied by all cat-dets. However, try not to be disheartened if you find it difficult. For accessibility, these sections will also be labelled with [(2)]{.alt}.
* `r user_astronaut(height = 1.5)` Those almost as brilliant as Mae (because no-one is quite as brilliant as her) get their own space suits so that they can go on space pirate adventures. They get to shout *RRRRRR* really loudly too. Actually, everyone here gets to should *RRRRRR* really loudly. Try it now. Go on. It feels good. Anyway, this material is the most advanced and you can consider it optional unless you are a postgraduate cat-det. For accessibility, these sections will also be labelled with [(3)]{.alt}.

It's not just me that's here to help though, you will meet other characters along the way:

* `r alien(height = 1.5)` aliens love dropping down onto the planet and probing humanoids. Unfortunately you'll find them probing you quite a lot with little coding challenges. Helps is at hand though. 
* `r robot(height = 1.5)` **bend-R** is our coding robot. She will help you to try out bits of `r rproj()` by writing the code for you before you encounter each coding challenge.
* `r bug(height = 1.5)` we also have our friendly alien bugs that will, erm, help you to avoid bugs in your code by highlighting common mistakes that even Mae Jemstone sometimes makes (but don't tell her I said that or my tuna supply will end). 

Also, use hints and solutions to guide you through the exercises (Figure 1).

<figure>
<img src="./images/discovr_hints.png" alt="Each codebox has a hints or solution button that activates a popup window containing code and text to guide you through each exercise." style="width:100%">
<figcaption>Figure 1: In a code exercise click the hints button to guide you through the exercise.</figcaption>
</figure> 
 

By for now and good luck - you'll be amazing!

### Workflow

* Before attempting this tutorial it's a good idea to work [this playlist of tutorials](https://youtube.com/playlist?list=PLEzw67WWDg83weG3idsgy4wuOIJAashA2&si=PiI-sDvqc1DkaWOq) on how to install, set up and work within `r rproj()` and `r rstudio()`.

* The tutorials are self-contained (you practice code in code boxes). However, so you get practice at working in `r rstudio()` I strongly recommend that you create an Quarto document within an `r rstudio()` project and practice everything you do in the tutorial in the Quarto document, make notes on things that confused you or that you want to remember, and save it. Within this Quarto document you will need to load the relevant packages and data. 

![](https://youtu.be/mqT7c17tofE)

### Packages

This tutorial uses the following packages:

* `broom` [@R-broom]
* `here` [@R-here]
* `knitr` [@R-knitr]
* `metafor` [@R-metafor; @metafor2010]

It also uses these `tidyverse` packages [@R-tidyverse; @tidyverse2019]: `dplyr` [@R-dplyr], `forcats` [@R-forcats], `ggplot2` [@wickhamGgplot2ElegantGraphics2016], and `readr` [@R-readr].

### Coding style

There are (broadly) two styles of coding:

1. **Explicit**: Using this style you declare the namespace (a.k.a. package name) when using a function: `package::function()`. For example, if I want to use the `mutate()` function from the package `dplyr`, I will type `dplyr::mutate()`. If you adopt an explicit style, you don't need to load packages at the start of your Quarto document (although see below for some exceptions).

2. **Concise**: Using this style you load all of the packages at the start of your Quarto document using `library(package_name)`, and then refer to functions without their namespace. For example, if I want to use the `mutate()` function from the package `dplyr`, I will use `library(dplyr)` in my first code chunk and thereafter refer to the function as `mutate()` when I use it.

Coding style is a personal choice. The [Google `r rproj()` style guide](https://google.github.io/styleguide/Rguide.html) and [tidyverse style guide](https://style.tidyverse.org/) recommend an explicit style, and I use it in teaching materials for two reasons (1) it helps you to remember which functions come from which packages, and (2) it prevents clashes resulting from using functions from different packages that have the same name. However, even with this style it makes sense to load `tidyverse` because the `dplyr` and `ggplot2` packages contain functions that are often used within other functions and in these cases explicit code is difficult to read. Also, no-one wants to write `ggplot2::` before every function from `ggplot2`.

You can use either style in this tutorial because all packages are pre-loaded. If working outside of the tutorial, load the `tidyverse` package (and any others if you're using a concise style) at the beginning of your Quarto document:

```{r eval = FALSE}
library(tidyverse)
```

### Data

To work *outside of this tutorial* you need to download the following data files:

* [brewin_2024.csv](https://milton-the-cat.rocks/data/csv/brewin_2024.csv)
* [brewin_es.csv](https://www.discovr.rocks/csv/brewin_es.csv)


Set up an `r rstudio()` project in the way that I recommend in this tutorial:

![](https://youtu.be/EA7JW2SfKSY?si=P2GUFb1q1AHWOpaW)

Then save the data files to the folder within your project called [data]{.alt}. Place this code in the first code chunk in your Quarto document:

```{r, eval=FALSE}
brewin_2024  <- here::here("data/brewin_2024.csv") |> readr::read_csv()
brewin_es <- here::here("data/brewin_es.csv") |> readr::read_csv()
```

## `r user_visor()` Overview of the statistical model [(2)]{.alt} {#theory}

### Fixed and random effect models

Broadly speaking there are two flavours of the meta-analysis model, both of which are variations on the linear model. The first is the **fixed-effects model**, in which studies in the meta-analysis are assumed to be sampled from a population with a fixed effect size or one that can be predicted from a few predictors. In this model there is one source of 'error', which is sampling variation. Thinking about this in terms of a linear model we have

$$
\begin{aligned}
d_k &= \delta + \varepsilon_k \\
\varepsilon &\sim N(0, \sigma^2)
\end{aligned}
$$

In other words, the effect size (Cohen's *d*) in study *k* (denoted $d_k$) is 'predicted from' or 'made up of', the effect size (size of *d*) in the population, which we denote as $\delta$, plus some sampling error, denoted as $\varepsilon_k$ (the standard notation for linear models). The usual assumption applies that the model errors follow a normal distribution with a mean of 0 (i.e. on average the error is zero) and a variance denoted $\sigma^2$. In this model, the effect size in each study is assumed to have been generated from a single population with a single effect size and the only reason that effect sizes vary across studies is because of sampling variance. Spoiler alert, this conceptualization probably isn't realistic [@field_is_2005].

The alternative is the **random-effects model**, in which studies in the meta-analysis are assumed to be sampled from different populations with effect sizes that themselves vary. In this model there are two sources of 'error': (1) sampling variation (denoted as $\varepsilon_k$); (2) variation between the true (population) effect size from which studies are taken (denoted as $\zeta_k$). In terms of a linear model we have

$$
\begin{aligned}
d_k &= \delta_k + \varepsilon_k \\
\delta_k &= \theta + \zeta_k\\
\varepsilon &\sim N(0, \sigma^2) \\
\zeta &\sim N(0, \tau^2)
\end{aligned}
$$

The first line looks the same as the first line of the previous model except that the population effect size has acquired a subscript of $k$. This difference is important and reflects the fact that in the random-effects model effect sizes within studies are not assumed to reflect a single population effect size but are instead assumed to have come from a population with an effect size that is different to the populations from which other studies in the meta-analysis were sampled. So now, the effect size in study *k* is made up of the effect size in population *k* plus some sampling error. The second line tells us that the population effect size for study *k* ($\delta_k$) is made up of the true effect size ($\theta$) and sampling error at the population level $\zeta_k$. The population-level sampling error is assumed to be normally distributed with a mean of 0 (i.e. on average the error is zero) and a variance denoted $\tau^2$. The random effects model can, therefore be thought of in terms of effect sizes within studies being sampled from populations which themselves are sampled from a 'superpopulation' that has a fixed effect size that represents the 'true' effect size of interest.

This model can be thought of as a hierarchical model with two levels of the hierarchy.

#### The level 1 (participant level) model

Ignoring the distributions of errors to simplify things, at level 1 (the bottom level) we have the participant-level model in which effect sizes for study $k$ are predicted from the effect size in the corresponding population and some sampling variation (at the sample level).

$$
\begin{aligned}
d_k &= \delta_k + \varepsilon_k \\
\end{aligned}
$$

#### The level 2 (study level) model

At level 1 (the top level) we have the study-level model in which effect sizes for population $k$ is predicted from the true effect size and some sampling variation (at the population level).


$$
\begin{aligned}
\delta_k &= \theta + \zeta_k\\
\end{aligned}
$$

#### The composite model

Rather than spread the model across two lines, we can equivalently write it as:

$$
\begin{aligned}
d_k &= \theta + \zeta_k + \varepsilon_k \\
\varepsilon &\sim N(0, \sigma^2) \\
\zeta &\sim N(0, \tau^2)
\end{aligned}
$$


<div class="infobox">

The primary role of meta-analysis is, therefore, to use the data from many studies to estimate the population effect size. This estimate ($\hat{\theta}$) is assumed to reflect the true size of the effect of interest.
  
</div>

### Moderators of effect sizes

In the previous section, we saw that the effect size for a study can be expressed as a linear model. In that model, the effect size is predicted from the population effect size and some sources of sampling variation. In effect these models are 'intercept-only' models. That is, the effect size for a study is predicted from only the intercept (i.e. an estimate of the true effect size). However, familiarity with linear models tells us that the basic meta-analytic model could be extended to include any number of predictors. Each predictor would be added to the model, and would be assigned a parameter that is estimated form the data. These parameters quantify the size and direction of the relationship between each predictor and the size of effect. In general, if we denote predictors with $X$ and their parameters with $\beta$ (i.e. commonly-used symbols for linear models), we can use the data to estimate a model that includes predictors:

$$
\begin{aligned}
d_k &= \theta + \beta_1X_k + \beta_2X_k + \ldots + \beta_nX_k + \zeta_k + \varepsilon_k \\
\varepsilon &\sim N(0, \sigma^2) \\
\zeta &\sim N(0, \tau^2)
\end{aligned}
$$

Predictors/moderators can be continuous or categorical variables. When a moderator is made up of several categories then it's possible to use any of the standard coding schemes such as dummy coding, contrast coding and so on. In this tutorial we use an example of dummy coding.


<div class="infobox">

A secondary role of meta-analysis is to test predictions that the size of effect will be associated with specific predictors relating to things like sample and methodological characteristics.

</div>

### Multiple effect sizes within studies

It is the norm rather than the exception that studies contribute more than one effect size to a meta-analysis. For example, the outcome of interest might have been measured using different questionnaires yielding a different effect sizes for each measure. Clinical trials often use multiple control groups yielding different effect sizes for the intervention group compared to each of the controls. 

We can handle this situation by adding a middle level to our hierarchy which acknowledges that effect sizes are clustered within studies. 


#### The level 1 (participant level) model

At level 1 (the bottom level) we again have the participant level model in which effect sizes for study $k$ are predicted from the effect size in the corresponding population and some sampling variation (at the sample level). This main difference from the random-effects model is that there are now two subscripts to acknowledge that each effect size *j* is nested within study *k*. In other words, it acknowledges the presence of an extra level in the hierarchy.

$$
\begin{aligned}
d_{jk} &= \delta_{jk} + \varepsilon_{jk} \\
\end{aligned}
$$


#### The level 2 (within study level) model

At level 2 (the middle level) we have the within-study model which acknowledges that each population effect size associated with effect size *j* within study *k* (\delta_{jk}) is itself made up of the effect size for the associated population, $\kappa_k$ and some sampling error (denoted as \zeta_{(2)jk} where the 2 tells us this is the error at level 2).

$$
\begin{aligned}
\delta_{jk} &= \kappa_k + \zeta_{(2)jk}\\
\end{aligned}
$$ 

Note that $\kappa_k$ does not have a *j* subscript because all of the *j* effect sizes within a study have the same population and, therefore, the same population effect size. However, different studies (and, remember, *k* denotes studies) have the same population and, therefore, the same population effect size so $\kappa$ varies across studies (*k*) but not across effect sizes within a study (*i*). 

The sampling error (denoted as $\zeta_{(2)jk}$) is assumed to be normally distributed with a mean of 0 and variance that we will denote as $\sigma^2_w$ with the *w* reminding us that this is *within*-study sampling error. The sampling error ($\zeta_{(3)k}$) reflects sampling variation across both effect sizes and studies and so has both *j* and *k* as subscripts.



#### The level 3 (between-study level) model

At level 3 (the top level) we have the study-level model in which effect sizes for population $k$ is predicted from the true effect size ($\theta$) and some sampling variation (at the population level).

$$
\begin{aligned}
\kappa_k &= \theta + \zeta_{(3)k}\\
\end{aligned}
$$

Note that $\theta$ has no subscripts because it is fixed (it is the true effect size and does not vary by study *k* or by effect size *j*). The sampling error (denoted as $\zeta_{(3)k}$) is assumed to be normally distributed with a mean of 0 and variance that we will denote as $\sigma^2_b$ with the *b* reminding us that this is *between*-study sampling error. The sampling error ($\zeta_{(3)k}$) reflects sampling variation across studies and so has a *k* subscript.



#### The composite model

Rather than spread the model across three lines, we can equivalently write it as follows, with the bottom three lines explicitly describing the distributions of the error terms/

$$
\begin{aligned}
d_{jk} &= \theta + \zeta_{(2)jk} + \zeta_{(3)k} + \varepsilon_{jk} \\
\varepsilon &\sim N(0, \sigma^2) \\
\zeta_{(2)} &\sim N(0, \sigma_w^2) \\
\zeta_{(3)} &\sim N(0, \sigma_b^2) \\
\end{aligned}
$$

In other words, effect size *j* within study *k* is made up of the true effect ($\theta$) plus sampling variation at the participant level ($\varepsilon_{jk}$), within each study ($\zeta_{(2)jk}$) and across studies ($\zeta_{(3)k}$)

#### Moderators

Within this extended framework we can add moderators of effect sizes in the same way as described earlier but the moderators can operate at the study level (all effect sizes within a study have the same value for a particular model) in which case they will have a *k* subscript only, or they can operate at the effect size level (effect sizes within a study can have different values of the same moderator in which case they have a *jk* subscript.) For example,

$$
\begin{aligned}
d_{jk} &= \theta + \beta_1X_k + \beta_2X_k + \ldots + \beta_nX_k + \zeta_{(2)jk} + \zeta_{(3)k} + \varepsilon_{jk} \\
\varepsilon &\sim N(0, \sigma^2) \\
\zeta_{(2)} &\sim N(0, \sigma_w^2) \\
\zeta_{(3)} &\sim N(0, \sigma_b^2) \\
\end{aligned}
$$


## `r bmu()` The study [(1)]{.alt} {#study}

@taylor_evidence_2022 reported that in healthy participants memories of traumatic and comparison films did not differ in coherence. However, @brewin_meta-analysis_2024 argued that there is convincing evidence for trauma memories in PTSD being incoherent or disorganized and conducted a meta-analysis to estimate the effect size between PTSD status and memory incoherence/disorganization. Key to the analysis is the fact that there are two distinct approaches to measuring disorganization/incoherence with acceptable face validity.

- [FOA]{.alt}: This method, based on therapy with PTSD patients, is to elicit a very detailed trauma narrative that includes the worst moments and have judges rate individual utterance units for markers of disorganization such as repetition and non-consecutive chunks.
- [Not FOA]{.alt}: This approach is based on judges or participants rating the entire memory or narrative (rather than specific details within in). This is the method employed by @taylor_evidence_2022 among others.

Specifically effect sizes were generated through one of four memory coding methods:

- [FOA (Detailed)]{.alt}: uses the FOA methodology with a detailed coding scheme
- [FOA (Global)]{.alt}: uses the FOA methodology but with global ratings coding scheme
- [Not FOA: (Disorganisation)]{.alt}: does not use the FOA methodology and uses measures of memory disorganisation
- [Not FOA (Organisation)]{.alt}: does not use the FOA methodology and uses measures of memory organisation (the direction of these effects had to be reversed before the meta-analysis)

This tutorial works through some parts of the meta-analysis conducted by @brewin_meta-analysis_2024. Specifically we look at conducting meta-analysis using Cohen's *d* as the effect size [@cohen_statistical_1988].


## `r user_visor()` The raw data [(2)]{.alt}

Usually for a meta-analysis you would input the raw data into a CSV file. The data for this study are in Table 1. Note the following things and use the scroll bars to help you see parts of the data

- **Each row represents information for an effect size and each column codes that information**: for example, Berntsen et al. (2003) in the first row contains information for a single effect size. The columns tell us things about that effect size such as that high disorganization is represented by a low score (`high_disorganization_is`), a memory questionnaire was used (`method`), memories were rated with a method that was not FOA (`foa`), the clinical group was PTSD (`clinical_group`), and the mean memory rating for the clinical group was 3.25 (`clinical_mean`) and 3.25 for controls (`control_mean`) and so on.
- Studies can contribute multiple effect sizes. For example, Evans et al (2007) contributes two effect sizes one is based on a global FOA assessment of memories and the other is based on a detailed FOA measure (see the columns `foa_type`,  `foa` and `foa_gp`, which combines the two into a single variable).
- Each study has a numeric ID variable (`study`)
- Each effect size (row) has a unique ID (`es_id`)
- For most studies we have the mean, standard deviation and sample size (*n*) of the memory rating for the clinical group and controls (in the variables `clinical_mean`, `clinical_sd`, `clinical_n`, `control_mean`, `control_sd`, and `control_n`). This is the information we need to compute Cohen's $d$.
- Sometimes studies don't include the information we need but report other information that can lead us to Cohen's $d$. For example, the paper might report Cohen's *d* directly, a *t*-statistic for the comparison of mean disorganization in clinical and control groups (`t` in our data), a *p*-value, or a correlation coefficient between PTSD symptoms (or group) and memory disorganization (`r` in our data). Note this kind of information for any study that reports it. 

```{r, echo = F}
brewin_2024 |> 
  dplyr::relocate(notes, .after = last_col()) |> 
  DT::datatable(caption = 'Table 1: Data from Brewin & Field (2024)',
                options = list(
                  autoWidth = TRUE,
                  scrollX = TRUE,
                  scrollY = "500px",
                  paginate = FALSE,
                  dom = 'tp',
                  columnDefs = list(
                    list(className = 'dt-center', targets = "_all"),
                    list(width = '2000px', targets = ncol(brewin_2024)),
                    list(width = '450px', targets = c(1, 3, 6))
                    ),
                  initComplete = htmlwidgets::JS(
                    "function(settings, json) {",
                    paste0("$(this.api().table().container()).css({'font-size': '", tbl_font_size, "'});"),
                    "}")
                  )
                )
```



## `r user_visor()` Computing effect sizes [(2)]{.alt}

### `r user_visor()` Working with descriptive statistics [(2)]{.alt}


Well start by using the raw data to calculate Cohen's *d* [@cohen_statistical_1988] using the `escalc()` function from `metafor` [@R-metafor; @metafor2010]. The `escalc()` function uses the information in the data to compute *d* and its variance and stores it in new columns/variables in your tibble. The raw data are stored in [brewin_2014]{.alt}.

#### `r robot()` Code example

The `escalc()` function takes the general form (including only the arguments relevant to us)

```{r, eval = F}
metafor::escalc(measure = "SMD",
                vtype = "UB",
                n1i = column_containing_exp_group_sample_size,
                n2i = column_containing_control_group_sample_size,
                m1i = column_containing_exp_group_mean,
                m2i = column_containing_control_group_mean,
                sd1i = column_containing_exp_group_sd,
                sd2i = column_containing_control_group_sd,
                ti = column_containing_t_statistic,
                pi = column_containing_p_value,
                di = column_containing_d_value,
                ri = column_containing_r_value,
                slab = column_containing_author_labels,
                var.names = c("variable_name_for_effect_size", "variable_name_for_effect_size_variance"),
                data = name_of_tibble) |> 
  as_tibble()
```

- The argument `measure = "SMD"` specifies the effect size measure to calculate and ["SMD"]{.alt} stands for standardized mean difference, in other words Cohen's *d*. In fact, Cohen's *d* is biased in small samples so the function automatically applies a correction and technically what we're working with is Hedges' *g* [@hedges_distribution_1981]. See [the technical appendix](#appendix).
- The argument `vtype = "UB"` specifies the type of sampling variance to calculate. The default is ["LS"]{.alt} but I suggest changing this to ["UB"]{.alt}, which gives an unbiased variance estimate [@hedges_random_1983]. See [the technical appendix](#appendix).
- For most studies we will estimate *d* from the mean, standard deviation and sample size (*n*) of the outcome measure for the group of interest (in this case the clinical group) and the controls. A series of arguments allow you to specify the variable sin the data that relate to this information.
    - `n1i` = specify the variable containing the sample size for the group of interest (in this case `clinical_n`)
    - `n2i` = specify the variable containing the sample size for the control/comparison group (in this case `control_n`)
    - `m1i` = specify the variable containing the mean outcome for the group of interest (in this case `clinical_mean`)
    - `m2i` = specify the variable containing the mean outcome for the control/comparison group (in this case `control_mean`)
    - `sd1i` = specify the variable containing the standard deviation of the outcome for the group of interest (in this case `clinical_sd`)
    - `sd2i` = specify the variable containing the standard deviation of the outcome for the control/comparison group (in this case `control_sd`)
- As mentioned above, sometimes studies don't include the information we need but we can get Cohen's $d$ in a different way. For example, if the paper reports a *t*-statistic for the comparison of mean disorganization in clinical and control groups (in the column `t`) we can convert this to *d*. There are arguments for obtaining *d* by other means:
  - `ti` = (if you have one) specify a variable containing *t*-statistics that compare your target group to the control on the outcome of interest (in this case `t`)
  - `pi` = (if you have one) specify a variable containing a *p*-statistics for a comparison of your target group to the control on the outcome of interest (we don't have such a variable in these data). Be careful with this argument, see [www.metafor-project.org/doku.php/tips:assembling_data_smd](https://www.metafor-project.org/doku.php/tips:assembling_data_smd)
  - `di` = (if you have one) specify a variable containing *d* for a comparison of your target group to the control on the outcome of interest (we don't have such a variable in these data)
  - `ri` = (if you have one) specify a variable containing *r* for the association between a measure of your grouping variable and the outcome (in this case `r`). I'd recommend [working with correlation coefficients manually](#rd).
- Some other useful options
  - `slab` = specify the variable that contains the text labels for the studies (in this case `author`)
  - `var.names` = specify names for the variables that `escalc` creates. I'd suggest `g`, `v_g` for the effect size and its variance respectively.
  - `data` = specify the name of the tibble you want to use (in this case `brewin_2024`).

For these data the final code will look like this:

```{r, eval = F}
brewin_2024g <- metafor::escalc(measure = "SMD",
                vtype = "UB",
                n1i = clinical_n,
                n2i = control_n,
                m1i = clinical_mean,
                m2i = control_mean,
                sd1i = clinical_sd,
                sd2i = control_sd,
                ti = t,
                slab = author,
                var.names = c("g", "v_g"),
                data = brewin_2024) |> 
  as_tibble()
```


Notice at the end I have piped the output into `as_tibble()` which converts the data to a tibble. I have stored this `tibbble` as  `brewin_2024g` (outside of this tutorial you could instead replace the original tibble by assigning the results to `brewin_2024`).

<div class="tip">
  `r cat_space()` **Tip**

  The `escalc()` function appends the effect sizes and their variances to your data, but doesn't export the augmented data as a tibble. We can coerce the output into a tibble by piping the results of `escalc()` into the `tibble::as_tibble()` function. For example,

```{r, eval = F, class.source = '.panel_alt'}
my_data_with_effect_sizes <- metafor::escalc(...) |> 
  as_tibble()
```
</div>

#### `r alien()` Alien coding challenge

Use the code above to create a tibble called `brewin_2024g` that adds Hedges' *g* and its variance to the `brewin_2024` tibble. Display the tibble to see what it looks like and check it has added the two new columns that you want.

```{r es_tib, exercise = TRUE, exercise.lines = 14}

```

```{r es_tib-solution}
brewin_2024g <- metafor::escalc(measure = "SMD",
                vtype = "UB",
                n1i = clinical_n,
                n2i = control_n,
                m1i = clinical_mean,
                m2i = control_mean,
                sd1i = clinical_sd,
                sd2i = control_sd,
                ti = t,
                slab = author,
                var.names = c("g", "v_g"),
                data = brewin_2024) |> 
  as_tibble()
brewin_2024g # show the data
```


### `r user_astronaut()` Working with correlations [(3)]{.alt} {#rd}

<div class="infobox">
  `r info()` **Information**

  Some studies that we wanted to use did not report means and standard deviations for the relevant groups and/or measures, but instead provided a correlation coefficient between PTSD symptoms and memory disorganization. This section describes how to handle converting between $r$ and $\hat{d}$, but is quite technical. For those who want to ignore it, `brewin_es` contains the data with all of the effect sizes included so you can move straight onto the next section.
</div>

There are two main types of Pearson correlation coefficient that you might want to convert to $\hat{d}$

- Standard *r*: The standard Pearson correlation measures the association between two continuous variables. In this example, if a study reported the correlation between PTSD symptoms and memory disorganization then this is a standard correlation. 
- Biserial *r*: The biserial correlation is the Pearson correlation between one continuous variable and one dichotomous variable (i.e. categorical variable made up of two categories). The dichotomous variable is assumed to have an underlying continuum. In this example, if a study reported the correlation between group membership (clinical or control) and memory disorganization then this is a biserial correlation. 

Where the relevant descriptive information is available the formulae in @mathur_simple_2020 can be used to convert a standard Pearson correlation coefficient into $\hat{d}$.

$$
\begin{aligned}
\hat{d} &= \frac{r\Delta}{s_x\sqrt{1-r^2}} \\
\widehat{SE}_\hat{d} &=  \left|\hat{d}\right|\sqrt{\frac{1}{r^2(N-3)} + \frac{1}{2(N-1)}}
\end{aligned}
$$

In which $r$ is the correlation coefficient, $s_x$ is the standard deviation of the predictor variable (in this case PTSD symptoms), $N$ is the total sample size on which $r$ is based and $\Delta$ specifies a change in the predictor that is of interest to the researcher. The standard error of $d$ uses the absolute value of the estimate of $\left|\hat{d}\right|$ the variance is the standard error.

This equation expresses the effect in terms of the change in the outcome variable resulting from an increase of $\Delta$ units in the continuous predictor measure. So, for these data if we set $\Delta = 10$, then the resulting $\hat{d}$ expresses the change in memory disorganization associated with a 10-unit increase in PTSD symptoms. 

To make the resulting $\hat{d}$ comparable to studies for which $d$ represented mean differences in disorganization between PTSD and control groups, $\Delta$ for a given study was set to be the difference between the mean PTSD scores in PTSD and control groups.

To apply this formula, the standard deviation of the PTSD measure ($s_x$) for the entire sample is required. Articles that did not report descriptive information for the PTSD measure for the entire sample did report it for PTSD and control subgroups allowing the whole sample variance ($s^2_\text{combined}$) to be estimated using [a standard formula](#appendix). Where this information was not available the formulae to convert a point biserial correlation can be used 

$$
\begin{aligned}
\hat{d} &= \frac{2r}{\sqrt{1-r^2}} \\
\widehat{SE}_\hat{d} &=  \frac{2}{\sqrt{(N-1)(1-r^2)}}.
\end{aligned}
$$

In effect, this is the same as applying the equation in @mathur_simple_2020 but with $\Delta = 2s_x$. In other words, it will yield the same value as the equation used for a continuous measure if PTSD and control groups had a mean difference on a continuous PTSD measure of about 2 standard deviations.

In both cases the variance of $\hat{d}$ is the [standard error squared]{#appendix}.

OK, so how do we implement this in `r rproj()`? This package has a couple of helper functions that you can use along with standard tidyverse functions.

- `pooled_var(nx, ny, sdx, sdy, ux, uy, sd = F)` calculates the pooled standard deviation of two groups based on specifying the variables that relate to the group sample sizes (`nx` and `ny`), standard deviations (`sx` and `sy`), and their means (`ux` and `uy`).
- `d_from_r(delta, sx, r)` converts a standard or biserial Pearson correlation to *d*. You need to specify the following arguments
  - `sx`: this needs to be set to the name of the variable containing the whole sample standard deviation of the predictor (in our data this variable is called `ptsd_sd_all`)
  - `r`: this needs to be set to the name of the variable containing the correlation coefficient
  - `delta`: set this to the value of delta that you want. In out case, we set this to be the difference in mean PTSD in the two groups. If this argument is not set the equation to convert a biserial *r* is used.
- `d_to_g` applies Hedge's correction to $d$
- `var_d_from_r` calculates the variance for Hedges' *g* based on a correlation coefficient.

#### `r robot()` Code example
  
Putting this all into practice leads us through the following gateway to coding hell:

```{r, eval = FALSE}
brewin_es <- brewin_2024g |> 
  dplyr::mutate(
    n_total = ifelse(is.na(clinical_n) | is.na(control_n), n_for_r, clinical_n + control_n),
    ptsd_sd_all = ifelse(is.na(ptsd_sd_all), metahelpr::pooled_var(nx = clinical_n, ny = control_n, sdx = ptsd_sd_clin, sdy = ptsd_sd_ctrl, ux = ptsd_mean_clin, uy = ptsd_mean_ctrl, sd = T), ptsd_sd_all),
    ptsd_diff = ptsd_mean_clin-ptsd_mean_ctrl,
    d_r = ifelse(!is.na(ptsd_diff), metahelpr::d_from_r(delta = ptsd_diff, sx = ptsd_sd_all, r = r), metahelpr::d_from_r(r = r)),
    g = ifelse(is.na(g), metahelpr::d_to_g(n = n_total, d = d_r), g), 
    g = ifelse(high_disorganization_is == "Low score", -g, g), 
    v_g = ifelse(is.na(v_g),  ifelse(!is.na(ptsd_diff), metahelpr::var_d_from_r(n = n_total, r = r, d = g),  metahelpr::var_d_from_r(n = n_total, r = r)), v_g)
    )
```

Let's translate this code in steps:

#### The general idea

The first thing we need to do is to add variables to `brewin_2024g` using `dplyr::mutate()` and then store this new version of the data as a new tibble called `brewin_es`. (If you prefer you can save it over the original tibble.) This is achieved using `brewin_es <- brewin_2024g |>  dplyr::mutate(...)`. The code *within* mutate controls what variables are created and added to the tibble.


<div class="tip">
  `r cat_space()` **Tip**

 Multiple times we use standard functions and operators that you might have met before.
 
 The `ifelse()` which takes the following format

```{r, eval = F, class.source = '.panel_alt'}
ifelse(condition_to_be_met,
       do_this_if_condition_is_met,
       do_this_if_condition_is_NOT_met)
```

Basically, within the function you set a condition, then you say what you'd like to happen if the condition is met, and then what you'd like to happen if it is not met.

The `is.na()` function tells us whether a given case of a variable is an `NA`.

The operator `!` negates something. For example, `!is.na()` tells us whether a given case of a variable is [not]{.alt} an `NA`.

The `|` operator means 'or'.

</div>

#### Within mutate

Within `mutate()` we have the following lines:

```{r, eval = F}
n_total = ifelse(is.na(clinical_n) | is.na(control_n),
                 n_for_r,
                 clinical_n + control_n)
```

This line creates a variable called `n_total` that will contain the total sample size for *r* ($N$ in the equations). For some studies we have this information (in the variable `n_for_r`), but for others we don't so we have to use `ifelse()` to set different rules depending on whether we have the sample sizes for each group.

The condition we set is `is.na(clinical_n) | is.na(control_n)` which means "if the variable `clinical_n` OR the variable `control_n` is an `NA`". We then tell `r rproj()` that when this condition is met, we set `n_total` to be the variable `n_for_r`, and when the condition is not met we set `n_total` to be the sum of the variables `clinical_n` and `control_n` (that is we add the group sample sizes together).

> Summary: this line creates a variable called `n_total` that is the sample size for *r*. If either `clinical_n` OR `control_n` are missing then the value of `n_for_r` is used, otherwise it is set to the sum of the clinical and control group sample sizes.

```{r, eval = F}
ptsd_sd_all = ifelse(is.na(ptsd_sd_all),
                     metahelpr::pooled_var(nx = clinical_n,
                                           ny = control_n,
                                           sdx = ptsd_sd_clin,
                                           sdy = ptsd_sd_ctrl,
                                           ux = ptsd_mean_clin,
                                           uy = ptsd_mean_ctrl,
                                           sd = T),
                     ptsd_sd_all)
```

This line creates a variable called `ptsd_sd_all` and we want this variable to contain the standard deviation of the PTSD measure (i.e. $s_x$). Sometimes we have this information from the study (stored in the variable `ptsd_sd_all`) and sometimes we'll need to calculate it from other information.

Again we `ifelse()` to set different rules depending on whether we need to calculate `ptsd_sd_all` or already have it. The condition we set is `is.na(ptsd_sd_all)` which means "if the variable `ptsd_sd_all` is an `NA`". In other words "is the value for `ptsd_sd_all` missing?". We then tell `r rproj()` that when this condition is met (i.e. the value is missing), use the function `pooled_var()` to calculate it, but if it is NOT missing then use the existing value.

> Summary: this line creates a variable called `ptsd_sd_all` that is the standard deviation of the PTSD measure (i.e. $s_x$). If this score doesn't already exist calculate it using `pooled_var()`, otherwise use the existing value.


```{r, eval = F}
ptsd_diff = ptsd_mean_clin - ptsd_mean_ctrl
```

This line creates a variable called `ptsd_diff`, which is the difference between the PTSD scores in the clinical group (`ptsd_mean_clin`) and the control group (`ptsd_mean_ctrl`). This tells us (on average) the gap in PTSD symptoms between the groups and will be used later to set $\Delta$.

```{r, eval = F}
d_r = ifelse(!is.na(ptsd_diff),
             metahelpr::d_from_r(delta = ptsd_diff, sx = ptsd_sd_all, r = r),
             metahelpr::d_from_r(r = r))
```

This line creates a variable called `d_r` which will contain values of *d* converted from *r*. Again we `ifelse()` to set different rules depending on whether we have enough information to calculate *d* from the standard *r* or we need to use the equations for biserial *r*. The condition we set is `!is.na(ptsd_diff)` which means "if the variable `ptsd_diff` is NOT an an `NA`". In other words "does the value for `ptsd_sd_all` exist?". We then tell `r rproj()` that when this condition is met (i.e. when `ptsd_sd_all` exist), use the function `d_from_r()` to calculate *d* setting delta equal to the value in `ptsd_sd_all`, otherwise treat *r* as a biserial correlation.

> Summary: this line creates a variable called `d_r` that contains values of *d* converted from *r*. Where we have the relevant information we estimate *d* treating *r* as a standard correlation, otherwise we treat *r* as a biserial correlation. 

```{r, eval = F}
g = ifelse(is.na(g), metahelpr::d_to_g(n = n_total, d = d_r), g), 
g = ifelse(high_disorganization_is == "Low score", -g, g)
```

This first line changes the existing variable `g` to insert the values of *d* we have calculated converted to Hedges' *g*. Again we `ifelse()` to set different rules depending on whether we already have a value of *g* in the data. The condition we set is `is.na(g)` which means "if the variable `g` is an `NA`". In other words "is the value for `g` missing?". We then tell `r rproj()` that when this condition is met (i.e. when `g` is missing), use the function `d_to_g()` to calculate *g* based on the value of `d_r` (calculated in the line above) and the total sample size (calculated earlier as `n_total`)

The second line is used to flip the direction of any effect sizes where the measure was 'organization' rather than 'disorganisation. Yet again, we use `ifelse()` and set the condition of `high_disorganization_is == "Low score"`. In other words, is the variable of the variable `high_disorganization_is` equal to `"Low score"`. When it is, we set `g` to be `-g` (i.e. we switch its sign), and when it's not we set `g` to be the existing value.

> Summary: the first lines changes the existing variable `g`to insert the values of *d* we have calculated but converted to Hedges' *g*. The second line reverses the direction of the effect *if* the variable `high_disorganization_is` tells us that this effect size is from a measure of organisation (rather than disorganization).

```{r, eval = FALSE}
v_g = ifelse(is.na(v_g), 
                 ifelse(!is.na(ptsd_diff),
                        metahelpr::var_d_from_r(n = n_total, r = r, d = g), 
                        metahelpr::var_d_from_r(n = n_total, r = r)),
                 v_g)
```

This line changes the existing variable `v_g` to insert the variance values for the *g*s we have calculated. This uses two nested `ifelse()` statements. The first `ifelse()` sets different rules depending on whether values for the variable `v_g` are missing. When they are missing, we move into the second `ifelse()` statement, but if they are not then we use the existing values. The second `ifelse()` statement sets different rules depending on whether `ptsd_diff` has a value When it was the variance is calculated assuming *r* was a standard Pearson correlation, and when not it is calculated assuming *r* was a biserial correlation.

> Summary: this code changes the existing variable `v_g`. If a value exists for `v_g` it is retained, but if it is missing it is computed in one of two ways. If a value of `ptsd_diff` exists then the variance is calculated assuming *r* was a standard Pearson correlation, and when not it is calculated assuming *r* was a biserial correlation.

#### `r alien()` Alien coding challenge

Replace the `XXXX` symbols and try running the code!

```{r get_brewin_2024g}
brewin_2024g <- metafor::escalc(measure = "SMD",
                vtype = "UB",
                n1i = clinical_n,
                n2i = control_n,
                m1i = clinical_mean,
                m2i = control_mean,
                sd1i = clinical_sd,
                sd2i = control_sd,
                ti = t,
                slab = author,
                var.names = c("g", "v_g"),
                data = brewin_2024) |> 
  as_tibble()
```


```{r get_r_from_d, exercise = TRUE, exercise.lines = 20, exercise.setup = "get_brewin_2024g"}
brewin_es <- brewin_2024g |> 
  dplyr::mutate(
    n_total = ifelse(is.na(XXXX) | is.na(XXXX),
                     n_for_r,
                     clinical_n + control_n),
    ptsd_sd_all = XXXX(is.na(ptsd_sd_all),
                       metahelpr::pooled_var(nx = clinical_n,
                                             ny = control_n,
                                             sdx = ptsd_sd_clin,
                                             sdy = XXXX,
                                             ux = ptsd_mean_clin,
                                             uy = ptsd_mean_ctrl,
                                             sd = T),
                       XXXX),
    ptsd_diff = ptsd_mean_clin - ptsd_mean_ctrl,
    d_r = ifelse(!is.na(XXXX),
                 metahelpr::d_from_r(delta = ptsd_diff, sx = XXXX, r = r),
                 metahelpr::d_from_r(r = XXXX)),
    g = ifelse(is.na(g),
               metahelpr::d_to_g(n = XXXX, d = d_r),
               g), 
    g = ifelse(high_disorganization_is == XXXX, -g, g), 
    v_g = ifelse(is.na(XXXX), 
                 ifelse(!is.na(ptsd_diff),
                        metahelpr::var_d_from_r(n = n_total, r = r, d = g), 
                        metahelpr::var_d_from_r(n = n_total, r = r)),
                 v_g)
    )

brewin_es
```

```{r get_r_from_d-solution}
brewin_es <- brewin_2024g |> 
  dplyr::mutate(
    n_total = ifelse(is.na(clinical_n) | is.na(control_n),
                     n_for_r,
                     clinical_n + control_n),
    ptsd_sd_all = ifelse(is.na(ptsd_sd_all),
                         metahelpr::pooled_var(nx = clinical_n,
                                    ny = control_n,
                                    sdx = ptsd_sd_clin,
                                    sdy = ptsd_sd_ctrl,
                                    ux = ptsd_mean_clin,
                                    uy = ptsd_mean_ctrl,
                                    sd = T),
                         ptsd_sd_all),
    ptsd_diff = ptsd_mean_clin-ptsd_mean_ctrl,
    d_r = ifelse(!is.na(ptsd_diff),
                 metahelpr::d_from_r(delta = ptsd_diff, sx = ptsd_sd_all, r = r),
                 metahelpr::d_from_r(r = r)),
    g = ifelse(is.na(g),
               metahelpr::d_to_g(n = n_total, d = d_r),
               g), 
    g = ifelse(high_disorganization_is == "Low score", -g, g), 
    v_g = ifelse(is.na(v_g), 
                 ifelse(!is.na(ptsd_diff),
                        metahelpr::var_d_from_r(n = n_total, r = r, d = g), 
                        metahelpr::var_d_from_r(n = n_total, r = r)),
                 v_g)
    )
brewin_es
```



## `r user_visor()` Fitting the random effects model [(2)]{.alt} {#re}

From now on we're going to use the data in [brewin_es]{.alt}, which is similar to the tibble you created in this tutorial except that I have removed some variables that we won't use.


### `r user_visor()` Aggregating the data [(2)]{.alt} {#agg}

You might have data where every study contributes only a single effect size. In which case skip this section. However, that's not the case for the data in this example. We can fit a model to the data as is, but if we want to create forest plots (more on them shortly) we will need a version of the data where we have aggregated within studies (or some other variable of interest).

#### `r robot()` Code example

To aggregate effect sizes within a variable we can use the `group_by()` and `summarize()` functions from `dplyr`. For example, this code

```{r, eval = F}
brewin_agg <- brewin_es |> 
  dplyr::group_by(author) |> 
  dplyr::summarize(
    g = mean(g, na.rm = T),
    v_g = mean(v_g, na.rm = T),
    n = mean(n_total, na.rm = T)
  )
```

takes the data in `brewin_es` and then groups it by the variable `author` (which identifies the study) and creates the variables `g`, `v_g` and `n` which are the average of the original variables of the same name. In other words, if a study has three values of `g` it will occupy three rows in the original tibble, but in this new tibble it will occupy 1 row and the value of `g` will be the average of the three values of `g`.

#### `r alien()` Alien coding challenge

To get a feel for this execute the code below and compare `brewin_es` with `brewin_agg`.

```{r ex_brewin_agg, exercise = TRUE, exercise.lines = 10}
brewin_agg <- brewin_es |> 
  dplyr::group_by(author) |> 
  dplyr::summarize(
    g = mean(g, na.rm = T),
    v_g = mean(v_g, na.rm = T),
    n = mean(n_total, na.rm = T)
  )
brewin_es # view the original data
brewin_agg # view the aggregated data
```

```{r ex_brewin_agg-solution}
brewin_agg <- brewin_es |> 
  dplyr::group_by(author) |> 
  dplyr::summarize(
    g = mean(g, na.rm = T),
    v_g = mean(v_g, na.rm = T),
    n = mean(n_total, na.rm = T)
  )
brewin_es # view the original data
brewin_agg # view the aggregated data
```

The current data are a bit more complicated than this because different measures of memory disorganization were used within some studies. The key hypothesis in this study is around how trauma memories were coded. This information is stored in the variable `foa_gp`. The aggregation we have just done loses all of the information about the type of memory coding used for a given effect size. 

#### `r robot()` Code example

If we want to aggregate taking the information in `foa_gp` into account we can add this variable to the `group_by()` function. The following code will aggregate effect sizes within each study, separately if they have different values of `foa_gp`. Note the only difference in this code is that we have added `foa_gp` into `group_by()` before `author`.

```{r, eval = F}
brewin_foa_agg <- brewin_es |> 
  dplyr::group_by(foa_gp, author) |> 
  dplyr::summarize(
    g = mean(g, na.rm = T),
    v_g = mean(v_g, na.rm = T),
    n = mean(n_total, na.rm = T)
  )
```



#### `r alien()` Alien coding challenge

Execute the code below and compare `brewin_es` with `brewin_agg` and `brewin_foa_agg` .

```{r ex_brewin_foa_agg, exercise = TRUE, exercise.lines = 10}
brewin_foa_agg <- brewin_es |> 
  dplyr::group_by(foa_gp, author) |> 
  dplyr::summarize(
    g = mean(g, na.rm = T),
    v_g = mean(v_g, na.rm = T),
    n = mean(n_total, na.rm = T)
  )
brewin_es # view the original data
brewin_agg # view the data aggregated within study
brewin_foa_agg # view the data aggregated within foa coding within study
```

```{r ex_brewin_foa_agg-solution}
brewin_foa_agg <- brewin_es |> 
  dplyr::group_by(foa_gp, author) |> 
  dplyr::summarize(
    g = mean(g, na.rm = T),
    v_g = mean(v_g, na.rm = T),
    n = mean(n_total, na.rm = T)
  )
brewin_es # view the original data
brewin_agg # view the data aggregated within study
brewin_foa_agg # view the data aggregated within foa coding within study
```


### `r user_visor()` One effect size per study [(2)]{.alt} {#rma}

When we have a single effect size per study (as we do in [brewin_agg]{.alt}), we can use the `rma()` function from `metafor` to fit a random effects model. The function has the following form

```{r, eval = F}
my_model <- rma(yi = variable_containing_effect_sizes,
                vi = variable_containing_effect_size_variances,
                mods = specify_moderator_variables,
                data = a_tibble,
                subset = specify_a_subset_of_the_tibble)
```

There are actually many more arguments, but the ones above are the only ones we need. We will look at [moderators](#mods) later, where we'll cover the [mods]{.alt} and [subset]{.alt} arguments. For now we will fit a no frills model.

#### `r robot()` Code example

For a no frills model we could use this code

```{r, eval = F}
brewin_ma <- rma(yi = g, vi = v_g, data = brewin_agg)
brewin_ma
```

which specifies the column containing effect sizes as [g]{.alt}, the column containing effect size variances as [v_g]{.alt} and the tibble containing the data as [brewin_agg]{.alt}, which is the tibble containing our aggregated data. The second line of code displays the results.


#### `r alien()` Alien coding challenge

Use the codebox below to fit a random effects model to the aggregated data in [brewin_agg]{.alt}.

```{r ex_brewin_ma, exercise = TRUE, exercise.lines = 3}

```

```{r ex_brewin_ma-solution}
brewin_ma <- rma(yi = g, vi = v_g, data = brewin_agg)
brewin_ma
```


```{r echo = F}
brewin_ma <- rma(yi = g, vi = v_g, data = brewin_agg)
```


The output contains several statistics:

- An estimate of $\tau^2$: this tells us the estimate of the variability in population effect sizes (see the theory section). In this case $\hat{\tau}^2$ = `r metafor::fmtx(brewin_ma$tau2, 2)`.
- The $I^2$ statistic is the percentage of the variability in effect sizes attributable to heterogeneity and not sampling error. Broad cut-offs of 25%, 50%, and 75% have been tentatively proposed for low, moderate, and high levels of heterogeneity [@higgins_measuring_2003], but these should not be applied thoughtlessly and context matters.  In this case $I^2$ = `r metafor::fmtx(brewin_ma$I2, 2)`%.
- The $H^2$ statistic is, essentially, a transformed and less intuitive version of $I^2$. It varies from 1 (homogeneity) to infinity (extreme heteroscedasticity). Probably just interpret $I^2$ and forget about $H^2$
- The $Q$ statistic tests the null hypothesis that effect sizes across studies are equal. A significant test can, therefore, be used to reject this null in favour of believing that heterogeneity exists. A non-significant test implies that heterogeneity wasn't sufficient;y large to be detected in the current sample. This test has notoriously low power, so non-significant results especially should be treated tentatively. In the current sample `r metahelpr::report_het(brewin_ma)`
- The main part of the output is the estimate of $\theta$ (using our earlier general notation), which in this case is our estimate of the population effect size $\hat{\theta} = \hat{g}$ (remembering that we converted our *d*s to *g*s before fitting the model). We are told the estimate itself, it's confidence interval and a significance test of the null that the effect size is zero. For these data, `r metahelpr::report_pars(brewin_ma)`. 

<div class="reportbox">
  `r pencil()` **Report it!**

  About $I^2$ = `r metafor::fmtx(brewin_ma$I2, 2)`% of variability in effect sizes was attributable to heterogeneity. With the caveats that this test is underpowered, the observed heterogeneity was not significantly different than zero, `r metahelpr::report_het(brewin_ma)`. Overall there was about a third of a standard deviation difference in memory disorganization scores between clinical and control groups, `r metahelpr::report_pars(brewin_ma)`. If we assume that this sample is one of the 95% that generates a confidence interval that captures the population value then this differences could be as small as `r metafor::fmtx(brewin_ma$ci.lb, 2)` standard deviations and as large as `r metafor::fmtx(brewin_ma$ci.ub, 2)` standard deviations. 
  
</div>



### `r user_visor()` Multiple effect sizes per study [(2)]{.alt} {#rmamv}

When we have multiple effect size per study (as we do in [brewin_es]{.alt}), we use the `rma.mv()` function from `metafor` to fit a random effects model. The function has the following form

```{r, eval = F}
my_model <- rma.mv(yi = variable_containing_effect_sizes,
                   V = variable_containing_effect_size_variances,
                   random = ~1|variable_identifying_study/variable_identifying_effect_size
                   mods = specify_moderator_variables,
                   data = a_tibble,
                   subset = specify_a_subset_of_the_tibble,
                   method = "REML")
```

The function is similar to `rma()` except that the effect size variance argument is named `V` rather than `vi`. One important difference is that we have to specify the nesting of the effect sizes within studies using the `random` argument. Essentially you need a numeric variable in the data that identifies each 'cluster' within which effect sizes are nested. Usually the 'cluster' is the study or article, but it could be something else. You also need a variable that uniquely identifies each effect size. In the current data `author` identifies each study and `es_id` identifies each effect size.

#### `r alien()` Alien coding challenge

Execute the following code, which selects the variables in the data that identify the studies, the effect size id, and the effect size itself. We view just the first 20 rows of the data.

```{r view_id_vars, exercise = TRUE, exercise.lines = 8}
brewin_es |> 
  dplyr::select(author, es_id, g) |> 
  head(20)
```

You can see that, for example, the study by Halligan et al. (2003) contributed 9 effect sizes to the data. These are identified both by the variable `author` (which tells us that these 9 effect sizes are clustered within the same study), and the variable `es_id` (which uniquely identifies the particular effect size).

To specify this nested structure we would, therefore, use

```{r eval = F}
random = ~1|author/es_id
```

Within `rma.mv()` we can specify whether to estimate the model parameters using the default of restricted maximum likelihood estimation ([method = "ML"]{.alt}) or full maximum likelihood, which is necessary if you want to compare nested models [method = "ML"]{.alt}. As with `rma()` there are many more arguments that we won't cover in this tutorial.


#### `r robot()` Code example

For a no frills model when each study contributes more than one effect size we could use this code

```{r, eval = F}
brewin_mamv <- rma.mv(yi = g, V = v_g, random = ~1|author/es_id, data = brewin_es)
brewin_mamv
```

which specifies the column containing effect sizes as [g]{.alt}, the column containing effect size variances as [v_g]{.alt}, the structure of the nesting of effect sizes as [~1|author/es_id]{.alt} and the tibble containing the data as [brewin_es]{.alt}, which is the tibble containing our aggregated data. The second line of code displays the results.


#### `r alien()` Alien coding challenge

Use the codebox below to fit a random effects model to the aggregated data in [brewin_agg]{.alt}.

```{r ex_brewin_mamv, exercise = TRUE, exercise.lines = 3}

```

```{r ex_brewin_mamv-solution}
brewin_mamv <- rma.mv(yi = g, V = v_g, random = ~1|author/es_id, data = brewin_es)
brewin_mamv
```

```{r, echo = F}
brewin_mamv <- rma.mv(yi = g, V = v_g, random = ~1|author/es_id, data = brewin_es)
```


The output contains several statistics:

- An estimate of $\sigma_b^2$: this tells us the estimate of the variability in population effect sizes (see the theory section). In this case $\hat{\sigma}_b^2$ = `r metafor::fmtx(brewin_mamv$sigma2[1], 3)`.
- An estimate of $\sigma_w^2$: this tells us the estimate of the variability in effect sizes within studies (see the theory section). In this case $\hat{\sigma}_w^2$ = `r metafor::fmtx(brewin_mamv$sigma2[2], 3)`.
- The $Q$ statistic tests the null hypothesis that effect sizes across studies are equal. A significant test can, therefore, be used to reject this null in favour of believing that heterogeneity exists. A non-significant test implies that heterogeneity wasn't sufficiently large to be detected in the current sample. This test has notoriously low power, so non-significant results especially should be treated tentatively. In the current sample `r metahelpr::report_het(brewin_mamv)`
- The main part of the output is the estimate of $\theta$ (using our earlier general notation), which in this case is our estimate of the population effect size $\hat{\theta} = \hat{g}$ (remembering that we converted our *d*s to *g*s before fitting the model). We are told the estimate itself, it's confidence interval and a significance test of the null that the effect size is zero. For these data, `r metahelpr::report_pars(brewin_mamv)`. 

<div class="reportbox">
  `r pencil()` **Report it!**
  
  For the overall model ignoring predictors, the between study variability was $\hat{\sigma}_b^2$ = `r metafor::fmtx(brewin_mamv$sigma2[1], 3)` and the within study variability was $\hat{\sigma}_w^2$ = `r metafor::fmtx(brewin_mamv$sigma2[2], 3)`. With the caveats that this test is underpowered, the observed heterogeneity was not significantly different than zero, `r metahelpr::report_het(brewin_mamv)`. Overall there was about a third of a standard deviation difference in memory disorganization scores between clinical and control groups, `r metahelpr::report_pars(brewin_mamv)`. If we assume that this sample is one of the 95% that generates a confidence interval that captures the population value then disorganization in PTSD groups could be anything between `r metafor::fmtx(brewin_mamv$ci.lb, 2)` and `r metafor::fmtx(brewin_mamv$ci.ub, 2)` standard deviations higher than in control groups.
</div>


It also possible to fir the model using robust estimates of the variances and covariances by placing the model into `metafor::robust()`. This function takes the form

```{r, eval = F}
metafor::robust(my_model, cluster = variable_identifying_study)
```

Basically, you place your model into the function and specify the variable that identifies the 'clusters' in the data (in the current context our clusters are the different studies included in the meta-analysis, which are identified by the variable `author`).

#### `r robot()` Code example

To get a robust version of the model we just created we would execute

```{r, eval = F}
metafor::robust(brewin_mamv, cluster = author)
```

#### `r alien()` Alien coding challenge

Use the codebox below to fit and view a robust version of the model [brewin_mamv]{.alt}, store this model as [brewin_rob]{.alt}.

```{r ex_brewin_rob-setup}
brewin_mamv <- rma.mv(yi = g, V = v_g, random = ~1|author/es_id, data = brewin_es)
```


```{r ex_brewin_rob, exercise = TRUE, exercise.lines = 3}

```

```{r ex_brewin_rob-solution}
brewin_rob <- metafor::robust(brewin_mamv, cluster = author)
brewin_rob
```

```{r, echo = F}
brewin_rob <- metafor::robust(brewin_mamv, cluster = author)
```

Compare the output to the non-robust model. The thing that changes is the standard error, which has changed from `r metafor::fmtx(brewin_mamv$se, 4)` to `r metafor::fmtx(brewin_rob$se, 4)`. This has a knock-on effect on the confidence interval and the test statistic used:

- Non-robust results: `r metahelpr::report_pars(brewin_mamv)`
- Robust results: `r metahelpr::report_pars(brewin_rob)`

## `r user_visor()` Forest plots [(2)]{.alt} {#forest}

A forest plot is a plot of each effect size and its confidence interval (usually) stacked vertically. The box displaying the effect size is scaled to reflect the precision of the effect size. Larger boxes indicate studies with greater sample sizes (and hence greater precision). In a meta-analysis studies based on larger samples (i.e. those with the least sampling error) are given more weight than smaller (less precise) studies. Therefore, the size of the box tells you something about the weight of effect size within the meta-analysis.  

The `metafor` package contains a very flexible system for creating forest plots (and other plots). At the core of this system is the `forest()` function. With flexibility comes a world of pain for trying to explain it all. So, to avoid that pain we'll look only at some of the key features of the `forest()` function. If you want to do anything more complicated then see some of the excellent examples at [https://www.metafor-project.org/doku.php/plots](https://www.metafor-project.org/doku.php/plots).

The `forest()` function takes the following form (with only a fraction of the numerous arguments listed).

```{r eval = F}
forest(my_model,
       slab = variable_identifying_unique studies,
       header = "Name for the list of studies",
       cex = numeric_value,
       cex.lab = numeric_value,
       xlab = "label for x-axis",
       xlim = c(lower_limit, upper_limit),
       at = seq(start, stop, step),
       shade = TRUE,
       mlab = metahelpr::forest_add_het(model = my_model))
```

Essentially you put your model created with `rma()` into `forest` and that's it unless you want to specify some options. So a basic forest plot for `brewin_ma` is created with `forest(brewin_ma)`. It's as simple as that. However, you will probably want to change some things. The best way to see what the options do is to change them and look at the effect.

#### `r alien()` Alien coding challenge

To see what the different arguments do, use the code box to work through the following steps to build up the code like above. At each step add the suggested argument in a new line (remember to put a comma between the arguments), [execute the code]{.alt} and see what effect it has on the plot. If you get stuck, the hints show the correct code for each step.

1. Plot a default plot with `forest(brewin_ma)`
2. By default the studies are listed as "Study 1", "Study 2" etc. Let's label them using the `author` variable from the data by adding the argument `slab = author`. The left hand column should now show the study names.
3. We can add a heading to the column with the study names by adding `header = "Study name"` (you can change the text in quotes to whatever you want your heading to be).
4. Everything looks a bit small. We can change this by scaling everything using `cex`. This is definitely an argument to play around with until everything looks how you want it. Try adding `cex = 0.6` but try other values to get a feel for what changing the value does.
5. The text below the *x*-axis (which says 'observed outcome') looks big. We can scale this separately using `cex.lab`. Try setting this to `cex.lab = 1`, but again try different values and note the effect.
6. The *x*-label of 'observed outcome' isn't informative, so we can set this using `xlab = "Memory disorganization"` (again change the text in quotes to what you want it to be).
7. The gap between the left and right columns and the plot in the middle can be reduced or expanded by setting the limits of the *x*-axis. The effect size data range from -1 to 2, so we could set limits of say -2.5 and 2.5 to see if this reduces the white space. Add the argument `xlim = c(-2.5, 2.5)` to do this (and again try different values and observe the effect when you execute the code).
8. We can set the intervals on the *x*-axis using `at`. The effect size data range from -1 to 2, so an interval of 0.5 might be good. If we include `at = seq(-1, 2, 0.5)` this will create ticke between -1 and 2 at intervals of 0.5 (that is, -1, -0.5, 0, 0.5, 1, 1.5, and 2).
9. If we include `shade = TRUE` alternate lines of the plot will be shaded which can make it easier to see which data relates to which study.
10. Finally, we can add a label at the bottom of the plot with `mlab`. This tutorial comes with a helper function (`forest_add_het()`) that adds heterogeneity statistics from a model to a forest plot. To add the heterogeneity statistics for our model ([brewin_ma]{.alt}) we need to include `mlab = metahelpr::forest_add_het(model = brewin_ma)`.

```{r forest-setup}
brewin_ma <- rma(yi = g, vi = v_g, data = brewin_agg)
```

```{r forest, exercise = TRUE, exercise.lines = 10}
# step 1:
forest(brewin_ma)
```

```{r forest-hint-1}
# step 2:
forest(brewin_ma,
       slab = author)
```

```{r forest-hint-2}
# step 3:
forest(brewin_ma,
       slab = author,
       header = "Study name")
```

```{r forest-hint-3}
# step 4:
forest(brewin_ma,
       slab = author,
       header = "Study name",
       cex = 0.6)
```

```{r forest-hint-4}
# step 5:
forest(brewin_ma,
       slab = author,
       header = "Study name",
       cex = 0.6,
       cex.lab = 1)
```

```{r forest-hint-5}
# step 6:
forest(brewin_ma,
       slab = author,
       header = "Study name",
       cex = 0.6,
       cex.lab = 1, 
       xlab = "Memory disorganization")
```

```{r forest-hint-6}
# step 7:
forest(brewin_ma,
       slab = author,
       header = "Study name",
       cex = 0.6,
       cex.lab = 1, 
       xlab = "Memory disorganization",
       xlim = c(-2.5, 2.5))
```

```{r forest-hint-7}
# step 8:
forest(brewin_ma,
       slab = author,
       header = "Study name",
       cex = 0.6,
       cex.lab = 1, 
       xlab = "Memory disorganization",
       xlim = c(-2.5, 2.5),
       at = seq(-1, 2, 0.5))
```

```{r forest-hint-8}
# step 9:
forest(brewin_ma,
       slab = author,
       header = "Study name",
       cex = 0.6,
       cex.lab = 1, 
       xlab = "Memory disorganization",
       xlim = c(-2.5, 2.5),
       at = seq(-1, 2, 0.5),
       shade = TRUE)
```

```{r forest-hint-9}
# step 10:
forest(brewin_ma,
       slab = author,
       header = "Study name",
       cex = 0.6,
       cex.lab = 1, 
       xlab = "Memory disorganization",
       xlim = c(-2.5, 2.5),
       at = seq(-1, 2, 0.5),
       shade = TRUE,
       mlab = metahelpr::forest_add_het(model = brewin_ma))
```
       
## `r user_visor()` Moderators (meta-regression) [(2)]{.alt} {#mods}

### `r user_visor()` The moderation model [(2)]{.alt} {#modmodel}

We saw in the [theory section](#theory) that you can add predictors to the model to see the extent to which effect sizes are predicted by characteristics associated with them. This is sometimes referred to as [meta-regression]{.alt} but I don't like this term because, although I can see why its used, it implies that a different model is being fitted when moderators are included to the model that estimates only the overall effect size (i.e. the intercept). In fact, the underlying model is the same all that changes is whether you include only the intercept (standard meta-analysis) or include predictors (moderator analysis/meta-regression). So, make me happy and don't refer to this as meta-regression.

In the current example there was a specific prediction that the way in which trauma memories were coded would predict the size of the effect observed. The variable `foa_gp` represents the way in which memories were coded for disorganization see the [study description](#study). As a brief reminder there were four ways in which memories were coded:

- [FOA (Detailed)]{.alt}: uses the FOA methodology with a detailed coding scheme
- [FOA (Global)]{.alt}: uses the FOA methodology but with global ratings coding scheme
- [Not FOA: (Disorganisation)]{.alt}: does not use the FOA methodology and uses measures of memory disorganisation
- [Not FOA (Organisation)]{.alt}: does not use the FOA methodology and uses measures of memory organisation (the direction of these effects had to be reversed before the meta-analysis)

The specific hypothesis was that the **FOA (Detailed)** method would generate larger effects than all other methods. Given we want to use a single category as a reference we can use standard dummy coding to fit the following model:

$$
\begin{aligned}
d_{jk} &= \theta + \beta_1(\text{FOA}_\text{Detailed} \text{ vs. FOA}_\text{Global})_k  + \beta_2(\text{FOA}_\text{Detailed} \text{ vs. Not FOA}_\text{Disorganisation})_k + \\
&\quad \beta_3(\text{FOA}_\text{Detailed} \text{ vs. Not FOA}_\text{Organisation})_k + \zeta_{(2)jk} + \zeta_{(3)k} + \varepsilon_{jk} \\
\varepsilon &\sim N(0, \sigma^2) \\
\zeta_{(2)} &\sim N(0, \sigma_w^2) \\
\zeta_{(3)} &\sim N(0, \sigma_b^2) \\
\end{aligned}
$$

In the data there are three variables representing these dummy variables

- `global_vs_detail`: is coded 0 for effect sizes relating to the FOA (detailed) method and 1 for effect sizes relating to the FOA (global) method.
- `dis_vs_detail`: is coded 0 for effect sizes relating to the FOA (detailed) method and 1 for effect sizes relating to the not FOA (disorganisation) method.
- `org_vs_detail`: is coded 0 for effect sizes relating to the FOA (detailed) method and 1 for effect sizes relating to the not FOA (organisation) method.

To include these predictors we assign them to the model as a formula using the `mods` argument of the `rma()` and `rma.mv()` functions. If you are familiar with the `lm()` function, then it's your lucky day because we use the same syntax. That is, if you want to add a single moderator the general syntax is (replacing `moderator_1` and `moderator_2` with the names of your variables):

```{r, eval = F}
mods = ~moderator_1
```

For two moderators with no interaction:

```{r, eval = F}
mods = ~moderator_1 + moderator_2
```

For two moderators with the interaction between them

```{r, eval = F}
mods = ~moderator_1*moderator_2
```

or equivalently

```{r, eval = F}
mods = ~moderator_1 + moderator_2 + moderator_1:moderator_2
```

and so on. We want to add in the variables `global_vs_detail`, `dis_vs_detail` and `org_vs_detail` so the `mods` argument will be

```{r, eval = F}
mods = ~global_vs_detail + dis_vs_detail + org_vs_detail
```


#### `r robot()` Code example

To add these three moderators to our model we would take the code we used to fit the model without the moderators:

```{r, eval = F}
brewin_mod <- rma.mv(yi = g,
                     V = v_g,
                     random = ~1|author/es_id, data = brewin_es)
```

and assign a formula indicating how the moderators should be included to the `mods` argument:

```{r, eval = F}
brewin_mod <- rma.mv(yi = g,
                     V = v_g,
                     mods = ~global_vs_detail + dis_vs_detail + org_vs_detail,
                     random = ~1|author/es_id, data = brewin_es)
```

As before, we get a robust version of this model by passing it into `robust` and specifying the variable representing the clusters (in this case studies):

```{r, eval = F}
brewin_rob <- robust(brewin_mod, cluster = author)
```


#### `r alien()` Alien coding challenge

Use the code box to fit a robust model with moderators and view the results.

```{r mods, exercise = TRUE, exercise.lines = 4}

```

```{r mods-hint-1}
# fit the model with moderators
brewin_mod <- rma.mv(yi = g,
                     V = v_g,
                     mods = ~global_vs_detail + dis_vs_detail + org_vs_detail,
                     random = ~1|author/es_id, data = brewin_es)
# Now get the robust model
```

```{r mods-hint-2}
# get the robust model
brewin_rob <- robust(brewin_mod, cluster = author)

# Now view the model
```

```{r mods-solution}
# put it all together

# fit the model with moderators
brewin_mod <- rma.mv(yi = g,
                     V = v_g,
                     mods = ~global_vs_detail + dis_vs_detail + org_vs_detail,
                     random = ~1|author/es_id, data = brewin_es)
# get the robust model
brewin_rob <- robust(brewin_mod, cluster = author)
# view the model
brewin_rob
```


```{r}
brewin_mod <- rma.mv(yi = g,
                     V = v_g,
                     mods = ~global_vs_detail + dis_vs_detail + org_vs_detail,
                     random = ~1|author/es_id, data = brewin_es)
brewin_rob <- robust(brewin_mod, cluster = author)
```


<div class="reportbox">
  `r pencil()` **Report it!**

The classification of effect sizes was dummy coded such that each category of effect size was compared to ones from FOA studies that quantified measures that used detailed coding. In the model including these dummy variables the between study variability was $\hat{\sigma}_b^2$ = `r metafor::fmtx(brewin_rob$sigma2[1], 3)` and the within study variability was $\hat{\sigma}_w^2$ = `r metafor::fmtx(brewin_rob$sigma2[2], 3)`. The observed heterogeneity was significantly different from zero, `r metahelpr::report_het(brewin_rob)`.

The overall moderation effect was significant, `r metahelpr::report_mod(brewin_rob)`. The parameter estimates indicated that compared to effect sizes relating to FOA studies that use detailed coding, effect sizes were (1) significantly larger in FOA studies that use global ratings, `r metahelpr::report_pars(brewin_rob, row = 2)`; (2) significantly smaller in not FOA studies that measured organisation, `r metahelpr::report_pars(brewin_rob, row = 3)`; and (3) not significantly different to not FOA studies that measured disorganisation, `r metahelpr::report_pars(brewin_rob, row = 4)`.
</div>

### `r user_visor()` Subgroup analysis [(2)]{.alt} {#subgroup}

The overall moderation effect was significant, `r metahelpr::report_mod(brewin_rob)`. It is common when this is true for a categorical moderator to run individual meta-analysis within each category of the moderator. In this case this would mean running four meta-analyses, one for each category of memory coding. We can do this using the `subgroup` argument of the `rma()` and `rma.mv()` functions. We use this argument by assigning a logical argument to it. For example, to fit a meta-analysis only using the effect sizes that used detailed FOA methodology, we would include:

```{r, eval = F}
subset = (foa_grp == "FOA (Detailed)")
```

The stuff in parenthesis is a logical argument that says 'include the effect size of the value of the variable `foa_gp` exactly matches the text string ["FOA (Detailed)"]{.alt}. You have to make sure that your text string *exactly* matches the label of the group.

#### `r robot()` Code example

To add fit a model only to effect sizes relating to detailed FOA coding methods we'd use

```{r, eval = F}
foa_detailed <- rma.mv(yi = g,
                     V = v_g,
                     subset = (foa_gp == "FOA (Detailed)"),
                     random = ~1|author/es_id, data = brewin_es)
```

#### `r alien()` Alien coding challenge

Use the code box to fit a robust model with moderators and view the results.

```{r foa_detailed, exercise = TRUE, exercise.lines = 5}

```

```{r foa_detailed-solution}
foa_detailed <- rma.mv(yi = g,
                     V = v_g,
                     subset = (foa_gp == "FOA (Detailed)"),
                     random = ~1|author/es_id, data = brewin_es)
foa_detailed
```

Obviously it's fairly tedious fitting separate models, which is why I have written a helper function (`metahelpr::get_mas()`) that fits models to individual categories and tabulates the results. It may or may not work because I'm not that good at writing functions. You're welcome. This function takes the general form 

```{r eval = F}
metahelpr::get_mas(tibble = name_of_your_tibble,
                   predictor = name_of_the_categorical_variable,
                   es = name_of_the_variable_containing_effect_sizes,
                   var_es = name_of_the_variable_containing_effect_size_variances,
                   es_id = name_of_the_variable_containing_effect_size_identifier, # optional
                   study_id = name_of_the_variable_containing_cluster_identifier, # optional
                   digits = number_of_decimal_places_to_use_in_table (default is 2),
                   p_digits = number_of_decimal_places_to_use_for_p_values (default is 3),
                   summary = TRUE or FALSE
)
```

Essentially, you put into the function the name of your tibble, the name of the variable you want the subgroup analysis for, and the names of the variables containing the effect sizes and their variances.

- If you specify variables for `es_id` and `study_id` then the function fits the models with `rma.mv()` otherwise `rma()` is used.
- By default a summary table is returned `summary = TRUE`, but if you'd rather have a tibble containing the data for each group, the models, and each models coefficients then use `summary = FALSE`. Changing this default is useful only if you want access to specific parts of the models.

<div class="tip">
  `r cat_space()` **Tip**

  There is also a function called `get_mod_mas()` which has the same arguments as `get_mas()` but also has a `moderator` argument where you can specify a single predictor variable.
</div>

#### `r alien()` Alien coding challenge

Execute the code below to produce a table of results for the subgroup analysis of the effect of the type of methodology used to code memories.

```{r subgroups, exercise = TRUE, exercise.lines = 10}
mod_tbl <- metahelpr::get_mas(tibble = brewin_es,
                   predictor = foa_gp,
                   es = g,
                   var_es = v_g,
                   es_id = es_id,
                   study_id = author)
mod_tbl
```

<div class="tip">
  `r cat_space()` **Tip**

  If working in `quarto`, pipe the output of `get_mas` into `knitr::kable` to get a nicely formatted table and you can add styling and captions in the usual way.

  ```{r, eval = F, class.source = '.panel_alt'}
mod_tbl |> knitr::kable()
```
</div>

You might sometimes want to specify a forest plot that is split by groups too. Again, I've got your back (well, assuming my dodgy functions work). This package also has a function called `metahelpr::forest_subgroups()`, which attempts to create a forest plot with effect sizes grouped by a categorical variable.

#### `r robot()` Code example

For the current data we'd use this code.

```{r eval = F}
metahelpr::forest_subgroups(tibble = brewin_foa_agg,
                 es = g,
                 var_es = v_g,
                 groups = foa_gp,
                 slab = author,
                 xlim = c(-2.5, 2.5),
                 at = seq(-1, 2.5, 0.5),
                 author_lab = "Study",
                 cex = 0.5,
                 cex.lab = 1,
                 xlab = "Memory disorganization",
                 shade = T)
```

The first four arguments require you to specify the data (`tibble = brewin_es`), the variable containing the effect sizes (`es = g`), the variable containing the effect size variances (`var_es = v_g`), the variable that you want to use to group effect sizes (`groups = foa_gp`) and the variable you want to use to label the studies (`slab = author`). All the other options allow you to feed values into the `forest()` function so see the section on [forest plots](#forest) for an explanation.


#### `r alien()` Alien coding challenge

The code below and play about with the arguments from `header` to `shade` to style the plot. Note I've used the data in `brewin_foa_agg` so that each study contributes a maximum of one effect size to each memory coding group on the plot. For example, Jones et al. (2007) contributes 9 effect sizes to *FOA detailed* and 3 to *FOA global* but in `brewin_foa_agg` these have been averaged so that we plot a single effect for each of those categories for Jones et al. (2007).

Change the data to `brewin_es` (where effect sizes have not been aggregated) and see the difference.

```{r sub_forest, exercise = TRUE, exercise.lines = 15}
metahelpr::forest_subgroups(tibble = brewin_foa_agg,
                 es = g,
                 var_es = v_g,
                 groups = foa_gp,
                 slab = author,
                 header = "Study name",
                 cex = 0.6,
                 cex.lab = 1, 
                 xlab = "Memory disorganization",
                 xlim = c(-2.5, 2.5),
                 at = seq(-1, 2, 0.5),
                 shade = TRUE)
```


## `r user_visor()` Writing up [(2)]{.alt}

This package contains some functions that, when working in quarto or Rmarkdown will help you to automatically extract and report certain results:

-   `report_het()`: collates information from heterogeneity tests and outputs text that summarizes the results in a format that will render nicely in quarto. Execute `?report_het` for help.
-   `report_mod`: outputs text that reports (and renders nicely in quarto/Rmarkdown) the omnibus statistical tests from a moderation model. Execute `?report_mod` for help.
-   `report_par_tbl`: outputs a tibble of the table of coefficients of a meta-analyses object (`rma` or `rma.mv`) created using the `metafor` package. This function will mostly be useful for models containing predictors of effect sizes (so-called meta-regression). Execute `?report_par_tbl` for help.
-   `report_pars`: outputs text that reports (and renders nicely in quarto/Rmarkdown) the individual effects from a meta-analysis model. Execute `?report_pars` for help.


## `r user_astronaut()` Statistical appendix [(3)]{.alt} {#appendix}

### Effect size calculations

Cohen's $\hat{d}$ [@cohen_statistical_1988] is estimated using the number of participants in each group ($n_1$ and $n_2$), the mean score for each group ($\overline{X}_1$ and $\overline{X}_2$) and the standard deviation for each group ($s_1$ and $s_2$):

$$
\begin{aligned}
\hat{d} &= \frac{\overline{X}_1-\overline{X}_2}{\sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 -2}}}
\end{aligned}
$$

The sampling variance of $d$ is

$$
SE_d = \sqrt{\frac{n_1+n_2}{n_1n_2} + \frac{\hat{d}^2}{2(n_1+n_2)}}.
$$

Estimates of $d$ are biased in small samples so were adjusted using a correction $J$ [@hedges_distribution_1981],

$$
J = 1-\frac{3}{4(n_1+n_2-2) - 1},
$$

such that

$$
\begin{aligned}
g &= J \times d \\
SE_g &= J \times SE_d.
\end{aligned}
$$

Note that the relationship between the standard error and variance is straightforward:

$$
\begin{aligned}
\sigma^2_g &= \sqrt{SE_g}.
\end{aligned}
$$


### Calculating the overall group variance

Where a sample is made up of two groups there is a standard formula to calculate the whole sample variance from the means, variances and sample sizes of the two groups

$$ 
s^2_\text{combined} = \frac{1}{n_1 + n_2-1}\bigg[(n_1-1)s_1^2 + (n_2-1)s_2^2 + \frac{n_1n_2}{n_1 + n_2}(\bar{x}_1-\bar{x}_2)^2\bigg]
$$

in which $n_1$, $\bar{x}_1$ and $s_1^2$ are the sample size, mean and standard deviation of the PTSD measure for the PTSD group and $n_2$, $\bar{x}_2$ and $s_2^2$ are the corresponding values for the control group. This equation is essentially the same as Result 1 @oneill_useful_2014. The helper function `pooled_var` applies this formula.






## Resources {data-progressive=FALSE}

### The `metafor` package

- There's loads of great material on the `metafor` [project website](https://www.metafor-project.org/)
- This section is really good on [plots and Figures](https://www.metafor-project.org/doku.php/plots) and in particular useful for doing more sophisticated forest plots
- This is the [package website](https://wviechtb.github.io/metafor/), which contains all of the help files in a searchable and nice format! 

### Statistics

* This is a great free online book on [doing meta-analysis using R](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/).
* There are free lectures and screencasts on my [YouTube channel](https://www.youtube.com/user/ProfAndyField/).
* There are free statistical resources on my websites [www.discoveringstatistics.com](http://www.discoveringstatistics.com) and [milton-the-cat.rocks](http://milton-the-cat.rocks).

### `r rproj()`

* [R for data science](http://r4ds.had.co.nz/index.html) by @wickhamDataScience2017 is an open-access book by the creator of the tidyverse (Hadley Wickham). It covers the *tidyverse* and data management.
* [`r rstudio()` cheat sheets](https://www.rstudio.com/resources/cheatsheets/).
* [`r rstudio()` list of online resources](https://www.rstudio.com/online-learning/).

### Acknowledgement

I'm extremely grateful to [Allison Horst](https://www.allisonhorst.com/) for her very informative blog post on [styling learnr tutorials with CSS](https://education.rstudio.com/blog/2020/05/learnr-for-remote/) and also for sending me a CSS template file and allowing me to adapt it. Without Allison, these tutorials would look a lot worse (but she can't be blamed for my colour scheme).

## References


